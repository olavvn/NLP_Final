# NLP_Final
NLP Final Report
20201036 Dohoon Kim(김도훈)
#데이터셋 준비 과정 
1.	초반에 계획했던 것은 카카오톡, 인스타그램의 모든 채팅 기록과 일주일 간 나의 발화를 녹음하여 텍스트로 변환한 것을 데이터로 사용하는 것이었음. 이떄 상대방의 채팅 혹은 발화가 아닌 나의 발화만을 사용하기로 했음. 그러나 Proposal 발표 이후, 온라인 상의 발화와 실제 나의 발화에 간극이 많이 존재할 것이라는 피드백을 받고, 카카오톡과 인스타그램의 채팅 기록만을 사용하기로 하였음.
2.	카카오톡과 인스타그램에서 채팅 기록을 모두 내려받았고, 하나의 파일로 통합한 후 나의 발화만을 추출하였음. 나의 발화에서도, 내가 직접 타이핑한 내용만 남기기 위하여 잉여 정보를 지우는 작업을 하였음(ex. URL, 이모티콘, 가져온 글, 카카오톡의 경우 송금 정보).
3.	이후 준비된 데이터를 바탕으로 학습을 시켰을 때, Language Model은 말이 안 되는 문장을 출력하였음. 이후 데이터셋을 다시 본 결과, 채팅방마다 드러나는 나의 말투가 매우 상이하였고, 이렇게 성격이 다른 데이터를 한꺼번에 학습시키니 정확도가 매우 떨어진 것이라고 판단하였음. 또한 ‘ㅜㅜ’, ‘ㅋㅋ’ 등의 반언어적인 표현이 빈번하게 등장하는 것 역시 학습에 지장을 주었을 것이라고 판단하였음.
4.	따라서 데이터셋의 크기를 대폭 감소시켰음. 기존에는 모든 대화방의 정보를 통합시켜 데이터셋을 만들었다면, 최종적으로는 친구 한 명과의 대화 기록만을 학습 데이터로 사용함. 내려 받은 대화 기록 중 가장 용량이 컸던 친구 A와의 대화만을 골랐고, 잉여 정보를 제거하는 과정을 거침. 이때 기존의 Language Model이 문맥상 맞지 않는 표현을 출력한 것을 보완하고자, 친구의 발화도 학습 데이터에 포함시킴. 
#모델 선정 및 학습 과정 
1.	Interim 발표시에는 Tokenizer와 Model 모두 GPT2를 사용했음. 그러나 Final에서는 한국어에 특화된 SKT-AI의 KoGPT2를 사용했음. 이 모델을 사용한 이유는, 이 모델은 한국어 데이터로 학습된 모델이기 때문에 한국어에 대한 Tokenizing을 잘 수행하고, 용량이 작아서 학습하는 데 시간이 오래 걸리지 않고, 무료임에도 성능이 좋기 때문임. 
2.	학습 시 친구 A의 발화와 그에 대응하는 나의 발화를 묶어서 학습을 진행함. Train set과 Test set의 비율을 9:1로 맞춰서 학습을 진행함. 
3.	데이터셋은 train set 1974개, test set 220개로 이루어짐. 상대적으로 작은 데이터셋을 이용하여 학습을 수행하기 위해 batch size를 4로 지정하고, epoch를 5로 지정하여 의도적으로 overfitting이 되게 함.
#평가 
1.	이전에 학습했을 때보다 loss가 줄었고, 문장을 생성하였을 때 문장 다운 문장이 나왔음. 친구 A와 대화 상황에서 대화 다운 대화를 할 수 있었음.
2.	다만 문맥을 이해하지는 못하였고, 과거 담화 내용을 기억하지는 못함.
3.	이 문제를 해결하기 위해 RAG(Retrieval-Augmented Generation) 및 Langchain 방식을 도입해 채팅 기록을 검색 가능한 데이터베이스로 만들면, 파인 튜닝 작업 없이 이미 존재하는 LLM모델을 이용하여 비슷한 작업을 수행할 수 있을 것임.
#제출자료(데이터셋 제출은 친구에게 동의를 받지 못하여 제출하지 못하는 점 양해 부탁드립니다.)

작업 노트북
https://colab.research.google.com/drive/1Fva9iTx3RGyEO4Rhiaubq08yq3YVWIqU?usp=sharing 
모델
https://drive.google.com/drive/folders/1Sd5f3zlERMx1PJWbQaj17SMMGfOJRjuS?usp=sharing
